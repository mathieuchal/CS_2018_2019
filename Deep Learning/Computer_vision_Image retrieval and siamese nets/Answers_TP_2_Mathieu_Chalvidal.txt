**Question 1**: What does each row of the matrix feats represent?
Each row of the matrix corresponds to the output of the network for one of the 5062 pictures of the datasets. It can be seen as a collapsed representation of this picture in the 4096-dimmensional generated by the network alexnet trained on imagenet.


**Question 2**: Where does the dimension of these lines comes from and how do we extract these features?
Since this Alexnet model is trained for a classification task, this 4096-dimensional vector corresponds actually to the last fully connected layer activation before the softmax activation of Alexnet. Thoses features are obtained through a serie of afines and non-linear transformations.
    

**Question 3**: What can be observe from the t-SNE visualization? Which classes 'cluster' well? Which do not?
PCA and T-SNE are 2 dimensionnality reduction algorithms that allows to collapse most of the dimension of a vectorial space while maintaining either the maximum explained variance on the dimension that are kept or the neighboring relations of the space. We observe on the visualisation that T-SNE yealds clusters of points that match more or less the different classes of the dataframe. For exemple radcliffe_camera (dark red) are very well clustered while all_souls for exemple spans the entire region of points. However, most of the classes aren't linearly separable with this representation, which account for the high number of false matches in the queries.
 

**Question 4**: Should we get better results? What should change? Why?    
Since the off-the-shelf network is trained on general images from Imagenet, its internal filters learn general visual patterns of the 1000 different classes. However, if we train the same model on a dataset that contains a lot of architectural patterns that matchs those that are present in the test dataset, its internal capacity of differenciation between classes will be more powerfull and the resulting embeddings will be far more separable. 


**Question 5**: Why do we change the last layer of the AlexNet architecture?
The dimension of the last layer has been decreased such that the final embedding dimension is 596. This decrease is due to the lower number of classes and visual patterns that are contained in our dataset. Hence, the semantic mapping that we nead to learn is more specific and by decreasing the number of dimensions of our embeddings we prevent our model to assign meaningless information in some dimensions that will lead to poorer similitude scoring. 


**Question 6**: How do we initialize the layers of model_1b for finetuning?   
We initialize all the layers of model_lb that matches those of model_la with the weights of model_la. Hence, our model will already have trained convolutional filters that are relevant for yealding a good representation of the image as output w.r.t to the information retrieval task.

**Question 7**: How does the visualization change after finetuning? What about the top results?
The querying results and the separation of the classes in the PCA/t-SNE visualisation has been improved by fine-tuning the model. However, some classes' cluster still remain not very well separated w.r.t to the others.

**Question 8**: Why images need to be resized to 224x224 before they can be fed to AlexNet? How can this affect results?
Since the initial model was trained with 224x224 images, we need our new images to abide with this shape size in order to leverage internal filters learnt with imageNet training. Otherwise, there would be shape distortion of those patterns that will affect the quality of the embeddings representation.

**Question 9**: Why does the size of the feature representation changes?
The feature representation relies now on the activation of the last feature filters layer. This layer is a Mean pooling along the 256 by 256 filters of the previous convolutional filter. Hence the feature representation is now of dimension 256 and is an average of the 256 filters of the previous layer.

**Question 10**: Why does the size of the feature representation is important for a image retrieval task?
The dimension of the representation impacts also the granularity of the semantic representation yealded by the model, that means the details of the image that are kept. Hence, when devising an architecture for the model, one should beer in mind the model task and its parameters, such as the number of classes or the need for capturing details for separability in order to achieve nuanced representations between classes such as in our exemple in the Oxford dataset.

**Question 11**: How does the aggregation layer changes the t-SNE visualization? 
Since T-SNE representation is explicitely optimized for replicating the neighboring relations betweens points in the high dimensional space (thanks to KL Divergence), better clusters in the high-dimensional space will yeald better clusters with t-SNE. 
That being said, since the activation consist in a average pooling along the filters dimension, it seems that the representation is now more robust to the internal variance of the clusters. Hence it seems logical to observe denser clusters for the classes but also some overlap that is again due to the mean pooling that "smooth" the expression of specific filters between classes.
    
**Question 12**: Can we see some structure in the clusters of similarly labeled images?
t-SNE does not preserve distance nor density of clusters but only account for local relation to neighboors of data point. Hence there would be no specific shape to expect from this visualisation, even though the clusters are more nicely shapped in the high-dimensional space.

**Question 13**: Why do we change the average pooling layer of the original Resnet18 architecture for a generalized mean pooling? 
Even though mean and max pooling operation has proven its efficiency in learning deep representations for image retrieval, it either makes the hypothesis that every activation is of equal importance or that the highest activation is the most important. Hence such pooling operation might plagued by the presence of uninformative image regions that play a negative role w.r.t to embedding representation or alternatively might be plagued by the obliteration of highly informative features in the case of max pooling. To tackle this problem and to strike a better tade-offer, Generalized mean pooling has been proposed. It consists in applying a average p-norm pooling operation. Many recent works have proven that embeddings using GeM improves image representation and boosts retrieval performance on standard benchmarks.

**Question 14**: What operation is the layer model_1d.adpool doing?
model_1d.adpool is adding a GeneralizedMeanPooling activation layer to the architecture of the network. This operation consist in applying an average p-norm pooling operation along every image feature yealded by the last convolutional layer. This p-norm can be set and is a trade-off between capturing information from very high activation filters while keeping the average information coming from the other features activation.


**Question 15**: How does this model compare with model 1c, that was trained in the same dataset for the same task?
**Question 16**: How does is compare to the finetuned models of 1b?
t-SNE vizualisation seems better than model_1c and model_1b w.r.t the separability of previously overlapping clusters. Queries results are conforting this observation as they consist mostly of images from the same classe.

**Question 17**: What can we say about the separation of data when included unlabeled images?
**Question 18**: And the distribution of the unlabeled features?
Including unlabelled data in the dataset yealds unseparable clusters as the unlabelled images representation distribution outputed by the networks span most of the regions of the classes clusters.

**Question 19**: How can we train a model to separate labeled from unlabeled data?
We could introduce an other Convolutional network which task will consist in discriminating between labeled and unlabeled data by capturing the very features that appears only in labeled data. To do so, we could define any convolutional architecture we want, and applying a single sigmoid activation at the top of our network to yeald a score of being labeled image or not. Then by formulating a cross-entropy loss function, we can optimize the network to differneciate between labeled and unlabeld data. We could do it as a whole or rather train one network for every classes which might yeald better results as the filters will be trained on very specific image feature each time.

**Question 19**: Compare the plots with unlabeled data of the model trained for retrieval (with triplet loss) and the model trained for classification of the previous subsection. How do they change?
Since the siamese network is explicitely optimized to differenciate the classes, we can observe a very good separation of the clusters with classes on the t-SNE embedding visualisation. However, it doesn't work any more with unlabeled data and the same problem of unseparability applies for classes such as commarket or balliol. 


**Question 20**: What is the difference in AP between a model that has trained with and without data augmentation?
Average precision score is a measure of the number of true positive over all positives when performing a classificaion. In this context, it represents the ability of the model to retrieve elements from a specific class for a given query. After attempting different queries, we see that Data augmentation increase almost uniformaly (on all classes) the average precision score. Our hypothesis is that Data Augmentation helps the model learn a more general classe representation that leads in capturing more unusual images of the classes, hence imroving average precision score.

**Question 21**: What about the clustering? Why do you believe some of the classes have not been adequately clustered yet?
Although some classes are clearly separated, other classes such as christ-church are split over the low-dimensional space and clusters for all_souls and magdalen are still overlaping. The first observation means that there are pole of images in those clusters that are very different from each other, hence leading in this spliting. The second is that some classes stil share embedded representation that are too close from each other.

**Question 22**: What other data augmentation or pooling techniques would you suggest to improve results? Why?

Other techniques can be used to further augment the Oxford dataset. Since it is composed of pictures taken outside of buildings, it is hugely dependant on brightness, shaddings and viewpoints, where there is a lot of variability inside classes from those perspectives. Hence playing on brightness, and sclaing images might help the model generalize better. We could also add some gaussian noise to images to prevent our model to learn high-frequency features leading to overfitting.

Additionally, several new data augmentation techniques have emerged with the rise of generative models. For instance, neural style transfer performed with GANs can be very helpful by augmenting our pictures over numerous combination of lightning and exposure. Altough this is a rather new technique. it might help generalization of the model by feeding a more complete classes description to the network.

Finally, in order to further enhance the pooling activation techniques that we disscussed previoulsy, we could propose to add learnable weights to the Generalized Mean Pooling in order for the network to draw attention on the very informative features activated by the previous layer w.r.t to the image matching task. Those weights are learned on activations altogether during the training process. This technique is proposed and further discussed in the paper "Weighted Generalized Mean Pooling for Deep Image Retrieval" (IEEE October 2018) 

**Question 24**: Why using a larger architecture results in a higher AP? Is this always going to be the case?
To some extent, a larger architecture will allow to capture a more complete representation of the image by offering a more comprehensive set of convolutional filters. This should offer a better capacity of capturing outliers of classes and then increasing de facto the average precision score by correctly matching them with there original classes. However, this holds up to a certain width and length of the architecture, where the informative signals coming from the image will be diluted in uninformative signals that will eventually lead in deteriorating the matching result. 




