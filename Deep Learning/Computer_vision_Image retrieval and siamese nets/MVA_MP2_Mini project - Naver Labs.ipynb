{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratical session on Image Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical session, we will explore how to perform typical tasks associated with image retrieval. Students will be able to download this IPython/Jupyter notebook after the class in order to perform the experiments also at home. \n",
    "\n",
    "**Link to the slides**: [PDF](https://www.dropbox.com/s/i79w8vbkgmvacof/25_02_MVA_practical_session.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a new copy of this notebook\n",
    "\n",
    "In order to follow this tutorial, please create a new copy of this notebook and name your copy using your own name. **Do not run this notebook directly, as it is read-only for students and any changes will not be able to be saved.** To start following this tutorial:\n",
    "\n",
    "\n",
    "1. Click File -> Make a Copy\n",
    "1. Wait for the new tab to open\n",
    "1. Click on the name of the notebook (probably Tutorial-CopyN) and rename it to 'Tutorial-yourname.lastname'.\n",
    "1. Close the previous tab containing this notebook (in order to avoid any mistakes when following the tutorial)\n",
    "1. Resume reading this tutorial from this point forward\n",
    "\n",
    "## Step 2: Basics for executing a Jupyter Notebook:\n",
    "\n",
    "In order to run a cell, select a cell and press **'shift + enter'**\n",
    "\n",
    "## Step 3: Write us back the answers of the questions below\n",
    "\n",
    "During the execution of this notebook, you will find some questions that need to be answered. Please write your answers in a separate text file and send us by e-mail at rafael.sampaio-de-rezende@naverlabs.com\n",
    " If you work together with other people during the practical session, you can send a single answer file for two or three people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparatives\n",
    "\n",
    "We start by importing the necessary modules and fixing a random seed. Please select the cell below and press **'shift+enter'**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import pdb\n",
    "import sys\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import sys   \n",
    "import warnings\n",
    "\n",
    "from datasets import create\n",
    "from archs import *\n",
    "from utils.test import extract_query\n",
    "from utils.tsne import do_tsne\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start by instantiating the Oxford dataset, that we will use in all following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Oxford 5k database\n",
    "dataset = create('Oxford')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query for some aspects of this dataset, such as the number of images, number of classes, the name of the different classes, and the class label for each of the images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Oxford\n",
      "\n",
      "Number of images:  2\n",
      "Number of classes: 0\n",
      "\n",
      "Class names: []\n"
     ]
    }
   ],
   "source": [
    "print('Dataset: ' + dataset.dataset_name)\n",
    "print()\n",
    "\n",
    "labels = dataset.get_label_vector()\n",
    "classes = dataset.get_label_names()\n",
    "\n",
    "print('Number of images:  ' + str(labels.shape[0]))\n",
    "print('Number of classes: ' + str(classes.shape[0]))\n",
    "print()\n",
    "print('Class names: ' + str(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load a list of models we can use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>queries</th>\n",
       "      <th>training</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexnet-cls-imagenet-fc7</th>\n",
       "      <td>data/features/alexnet-cls-imagenet-fc7_ox.npy</td>\n",
       "      <td>data/features/alexnet-cls-imagenet-fc7_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexnet-cls-lm-fc7</th>\n",
       "      <td>data/features/alexnet-cls-lm-fc7_ox.npy</td>\n",
       "      <td>data/features/alexnet-cls-lm-fc7_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexnet-cls-lm-gem</th>\n",
       "      <td>data/features/alexnet-cls-lm_ox.npy</td>\n",
       "      <td>data/features/alexnet-cls-lm_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet18-cls-imagenet-gem</th>\n",
       "      <td>data/features/resnet18-cls-imagenet_ox.npy</td>\n",
       "      <td>data/features/resnet18-cls-imagenet_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet18-cls-imagenet-gem-pcaw</th>\n",
       "      <td>data/features/resnet18-cls-imagenet-pca_ox.npy</td>\n",
       "      <td>data/features/resnet18-cls-imagenet-pca_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet18-cls-lm-gem</th>\n",
       "      <td>data/features/resnet18-cls-lm_ox.npy</td>\n",
       "      <td>data/features/resnet18-cls-lm_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet18-cls-lm-gem-pcaw</th>\n",
       "      <td>data/features/resnet18-cls-lm-pca_ox.npy</td>\n",
       "      <td>data/features/resnet18-cls-lm-pca_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet18-rnk-lm-gem</th>\n",
       "      <td>data/features/resnet18-rnk-lm_ox.npy</td>\n",
       "      <td>data/features/resnet18-rnk-lm_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet18-rnk-lm-gem-da</th>\n",
       "      <td>data/features/resnet18-rnk-lm-da_ox.npy</td>\n",
       "      <td>data/features/resnet18-rnk-lm-da_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/models/resnet18-rnk-lm-da.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet18-rnk-lm-gem-da-mr</th>\n",
       "      <td>data/features/resnet18-rnk-lm-da_mr_ox.npy</td>\n",
       "      <td>data/features/resnet18-rnk-lm-da_mr_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/models/resnet18-rnk-lm-da.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50-cls-imagenet-gem</th>\n",
       "      <td>data/features/resnet50-cls-imagenet_ox.npy</td>\n",
       "      <td>data/features/resnet50-cls-imagenet_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/models/resnet50-cls-imagenet.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50-cls-imagenet-gem-pcaw</th>\n",
       "      <td>data/features/resnet50-cls-imagenet-pca_ox.npy</td>\n",
       "      <td>data/features/resnet50-cls-imagenet-pca_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50-cls-lm-gem</th>\n",
       "      <td>data/features/resnet50-cls-lm_ox.npy</td>\n",
       "      <td>data/features/resnet50-cls-lm_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50-cls-lm-gem-pcaw</th>\n",
       "      <td>data/features/resnet50-cls-lm-pca_ox.npy</td>\n",
       "      <td>data/features/resnet50-cls-lm-pca_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50-rnk-lm-gem</th>\n",
       "      <td>data/features/resnet50-rnk-lm_ox.npy</td>\n",
       "      <td>data/features/resnet50-rnk-lm_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/models/resnet50-rnk-lm.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50-rnk-lm-gem-da</th>\n",
       "      <td>data/features/resnet50-rnk-lm-da_ox.npy</td>\n",
       "      <td>data/features/resnet50-rnk-lm-da_oxq.npy</td>\n",
       "      <td>data/features/resnet50-rnk-lm-da_lmforPQ.npy</td>\n",
       "      <td>data/models/resnet50-rnk-lm-da.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50-rnk-lm-gem-da-mr</th>\n",
       "      <td>data/features/resnet50-rnk-lm-da_mr_ox.npy</td>\n",
       "      <td>data/features/resnet50-rnk-lm-da_mr_oxq.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/models/resnet50-rnk-lm-da.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       dataset  \\\n",
       "alexnet-cls-imagenet-fc7         data/features/alexnet-cls-imagenet-fc7_ox.npy   \n",
       "alexnet-cls-lm-fc7                     data/features/alexnet-cls-lm-fc7_ox.npy   \n",
       "alexnet-cls-lm-gem                         data/features/alexnet-cls-lm_ox.npy   \n",
       "resnet18-cls-imagenet-gem           data/features/resnet18-cls-imagenet_ox.npy   \n",
       "resnet18-cls-imagenet-gem-pcaw  data/features/resnet18-cls-imagenet-pca_ox.npy   \n",
       "resnet18-cls-lm-gem                       data/features/resnet18-cls-lm_ox.npy   \n",
       "resnet18-cls-lm-gem-pcaw              data/features/resnet18-cls-lm-pca_ox.npy   \n",
       "resnet18-rnk-lm-gem                       data/features/resnet18-rnk-lm_ox.npy   \n",
       "resnet18-rnk-lm-gem-da                 data/features/resnet18-rnk-lm-da_ox.npy   \n",
       "resnet18-rnk-lm-gem-da-mr           data/features/resnet18-rnk-lm-da_mr_ox.npy   \n",
       "resnet50-cls-imagenet-gem           data/features/resnet50-cls-imagenet_ox.npy   \n",
       "resnet50-cls-imagenet-gem-pcaw  data/features/resnet50-cls-imagenet-pca_ox.npy   \n",
       "resnet50-cls-lm-gem                       data/features/resnet50-cls-lm_ox.npy   \n",
       "resnet50-cls-lm-gem-pcaw              data/features/resnet50-cls-lm-pca_ox.npy   \n",
       "resnet50-rnk-lm-gem                       data/features/resnet50-rnk-lm_ox.npy   \n",
       "resnet50-rnk-lm-gem-da                 data/features/resnet50-rnk-lm-da_ox.npy   \n",
       "resnet50-rnk-lm-gem-da-mr           data/features/resnet50-rnk-lm-da_mr_ox.npy   \n",
       "\n",
       "                                                                        queries  \\\n",
       "alexnet-cls-imagenet-fc7         data/features/alexnet-cls-imagenet-fc7_oxq.npy   \n",
       "alexnet-cls-lm-fc7                     data/features/alexnet-cls-lm-fc7_oxq.npy   \n",
       "alexnet-cls-lm-gem                         data/features/alexnet-cls-lm_oxq.npy   \n",
       "resnet18-cls-imagenet-gem           data/features/resnet18-cls-imagenet_oxq.npy   \n",
       "resnet18-cls-imagenet-gem-pcaw  data/features/resnet18-cls-imagenet-pca_oxq.npy   \n",
       "resnet18-cls-lm-gem                       data/features/resnet18-cls-lm_oxq.npy   \n",
       "resnet18-cls-lm-gem-pcaw              data/features/resnet18-cls-lm-pca_oxq.npy   \n",
       "resnet18-rnk-lm-gem                       data/features/resnet18-rnk-lm_oxq.npy   \n",
       "resnet18-rnk-lm-gem-da                 data/features/resnet18-rnk-lm-da_oxq.npy   \n",
       "resnet18-rnk-lm-gem-da-mr           data/features/resnet18-rnk-lm-da_mr_oxq.npy   \n",
       "resnet50-cls-imagenet-gem           data/features/resnet50-cls-imagenet_oxq.npy   \n",
       "resnet50-cls-imagenet-gem-pcaw  data/features/resnet50-cls-imagenet-pca_oxq.npy   \n",
       "resnet50-cls-lm-gem                       data/features/resnet50-cls-lm_oxq.npy   \n",
       "resnet50-cls-lm-gem-pcaw              data/features/resnet50-cls-lm-pca_oxq.npy   \n",
       "resnet50-rnk-lm-gem                       data/features/resnet50-rnk-lm_oxq.npy   \n",
       "resnet50-rnk-lm-gem-da                 data/features/resnet50-rnk-lm-da_oxq.npy   \n",
       "resnet50-rnk-lm-gem-da-mr           data/features/resnet50-rnk-lm-da_mr_oxq.npy   \n",
       "\n",
       "                                                                    training  \\\n",
       "alexnet-cls-imagenet-fc7                                                 NaN   \n",
       "alexnet-cls-lm-fc7                                                       NaN   \n",
       "alexnet-cls-lm-gem                                                       NaN   \n",
       "resnet18-cls-imagenet-gem                                                NaN   \n",
       "resnet18-cls-imagenet-gem-pcaw                                           NaN   \n",
       "resnet18-cls-lm-gem                                                      NaN   \n",
       "resnet18-cls-lm-gem-pcaw                                                 NaN   \n",
       "resnet18-rnk-lm-gem                                                      NaN   \n",
       "resnet18-rnk-lm-gem-da                                                   NaN   \n",
       "resnet18-rnk-lm-gem-da-mr                                                NaN   \n",
       "resnet50-cls-imagenet-gem                                                NaN   \n",
       "resnet50-cls-imagenet-gem-pcaw                                           NaN   \n",
       "resnet50-cls-lm-gem                                                      NaN   \n",
       "resnet50-cls-lm-gem-pcaw                                                 NaN   \n",
       "resnet50-rnk-lm-gem                                                      NaN   \n",
       "resnet50-rnk-lm-gem-da          data/features/resnet50-rnk-lm-da_lmforPQ.npy   \n",
       "resnet50-rnk-lm-gem-da-mr                                                NaN   \n",
       "\n",
       "                                                             weights  \n",
       "alexnet-cls-imagenet-fc7                                         NaN  \n",
       "alexnet-cls-lm-fc7                                               NaN  \n",
       "alexnet-cls-lm-gem                                               NaN  \n",
       "resnet18-cls-imagenet-gem                                        NaN  \n",
       "resnet18-cls-imagenet-gem-pcaw                                   NaN  \n",
       "resnet18-cls-lm-gem                                              NaN  \n",
       "resnet18-cls-lm-gem-pcaw                                         NaN  \n",
       "resnet18-rnk-lm-gem                                              NaN  \n",
       "resnet18-rnk-lm-gem-da             data/models/resnet18-rnk-lm-da.pt  \n",
       "resnet18-rnk-lm-gem-da-mr          data/models/resnet18-rnk-lm-da.pt  \n",
       "resnet50-cls-imagenet-gem       data/models/resnet50-cls-imagenet.pt  \n",
       "resnet50-cls-imagenet-gem-pcaw                                   NaN  \n",
       "resnet50-cls-lm-gem                                              NaN  \n",
       "resnet50-cls-lm-gem-pcaw                                         NaN  \n",
       "resnet50-rnk-lm-gem                   data/models/resnet50-rnk-lm.pt  \n",
       "resnet50-rnk-lm-gem-da             data/models/resnet50-rnk-lm-da.pt  \n",
       "resnet50-rnk-lm-gem-da-mr          data/models/resnet50-rnk-lm-da.pt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dictionary of the available models and features\n",
    "with open('data/models.json', 'r') as fp:\n",
    "    models_dict = json.load(fp)\n",
    "\n",
    "pd.DataFrame(models_dict).T # show the loaded models onscreen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Training\n",
    "\n",
    "In this first part of the tutorial, we will study how different changes in the training pipeline (e.g. choice of model, pooling, and post-processing options) can change the quality of results we obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Creating a network with the AlexNet architecture\n",
    "\n",
    "As a first step, we will be creating a neural network implementing the AlexNet architecture to use in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantate the model for the first experiment\n",
    "model_1a = alexnet_imagenet()\n",
    "\n",
    "# show the network details\n",
    "print(model_1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could use this model to extract features for all images in our dataset. In order to make this faster, we have already precomputed those features and stored them in the disk.\n",
    "\n",
    "In order to load the features computed by this model from the disk, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/features/alexnet-cls-imagenet-fc7_ox.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7e6c831ed839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alexnet-cls-imagenet-fc7'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/features/alexnet-cls-imagenet-fc7_ox.npy'"
     ]
    }
   ],
   "source": [
    "dfeats = np.load(models_dict['alexnet-cls-imagenet-fc7']['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: What does each row of the matrix feats represent?\n",
    "\n",
    "**Question 2**: Where does the dimension of these lines comes from and how do we extract these features?\n",
    "\n",
    "_Hint: if you do not know the answers for the questions above, try running the following command:_\n",
    "```\n",
    "model_1a_test = alexnet_imagenet_fc7(); print(model_1a_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assuming that we have already used our network to extract features from all images in the dataset and stored them in the matrix *dfeats* (as done above), we will retrieve the top-15 images that are most similar to a query image. In our example, we will use the following image as a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_idx = 11 # feel free to switch to another number afterwards, but test first with 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize top results for a given query\n",
    "dataset.vis_top(dfeats, q_idx, ap_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the right of the query image, we plot the best retrieval results, with decreasing similarity from left to right. Images in green frames are **true matches**, red frames are **false matches**, and gray frames are so-called **'junk' matches** (images from the same landmark, but from angles too different or at wrong spots). Junk matches are ignored during the calculation of the AP.\n",
    "\n",
    "Now we will use the t-SNE algorithm to cluster images together according to feature similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: What can be observe from the t-SNE visualization? Which classes 'cluster' well? Which do not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Finetuning the created network on the Landmarks dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see what happens when we fine-tune our off-the-shelf ImageNet network in the Landmarks dataset and then repeat the process above. \n",
    "\n",
    "We can quickly compare some exemples of images of both training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('figs/imagenet_ex.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('figs/lm_ex.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Should we get better results? What should change? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1b = alexnet_lm() # instantate the model that has been fine-tuned in landmarks\n",
    "\n",
    "print(model_1b) # show the network details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare with the model we had before:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: Why do we change the last layer of the AlexNet architecture?\n",
    "\n",
    "**Question 6**: How do we initialize the layers of model_1b for finetuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now repeat the same process we had done before, but now using image features that have been extracted using the fine-tuned network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeats = np.load(models_dict['alexnet-cls-lm-fc7']['dataset'])\n",
    "pd.DataFrame(dfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the top-15 most similar images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.vis_top(dfeats, q_idx, ap_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**: How does the visualization change after finetuning? What about the top results?\n",
    "\n",
    "**Question 7**: Why images need to be resized to 224x224 before they can be fed to AlexNet? How can this affect results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Replacing last max pooling layer with GeM layer\n",
    "\n",
    "Now, we will replace the last max pooling layer of our network with a GeM layer and see how this affects the results. For this model, we remove all fully connected layers (classifier layers) and replace the last max pooling layer by an aggregation pooling layer (more details about this layer in the next subsection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1c = alexnet_GeM() # instantate the fine-tuned model with a GeM layer instead of max-pooling\n",
    "\n",
    "print(model_1c) # show the network details. Can you identify what has changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare with the model we had before:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume again we have used this model to extract features from all images and stored them in the *dfeats* variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeats = np.load(models_dict['alexnet-cls-lm-gem']['dataset'])\n",
    "\n",
    "pd.DataFrame(dfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**: Why does the size of the feature representation changes?\n",
    "\n",
    "**Question 9**: Why does the size of the feature representation is important for a image retrieval task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's continue visualizing the top-15 most similar images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.vis_top(dfeats, q_idx, ap_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**: How does the aggregation layer changes the t-SNE visualization? \n",
    "    \n",
    "**Question 11**: Can we see some structure in the clusters of similarly labeled images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) ResNet18 architecture with GeM pooling\n",
    "\n",
    "Now, we will replace the base architecture of our network (the backbone) with a ResNet18 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = resnet18()      # instantiate one model with average pooling and another \n",
    "model_1d = resnet18_GeM() # with GeM pooling with the same ResNet18 architecture\n",
    "\n",
    "print(model_0.adpool)     # Show how the last layers of the two models are different\n",
    "print(model_1d.adpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12**: Why do we change the average pooling layer of the original Resnet18 architecture for a generalized mean pooling? \n",
    "\n",
    "**Question 13**: What operation is the layer model_1d.adpool doing?\n",
    " * _Hint: You can see the code of the generalized mean pooling in file modules.py_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same as before and visualize the features and top-15 most similar images to our query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a different image for testing this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_idx = 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load Oxford features from ResNet18 model and visualize the top-15 results for the given query index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeats = np.load(models_dict['resnet18-cls-lm-gem']['dataset'])\n",
    "qfeats = np.load(models_dict['resnet18-cls-lm-gem']['queries'])\n",
    "dataset.vis_top(dfeats, q_idx, q_feat=qfeats[q_idx], ap_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 14**: How does this model compare with model 1c, that was trained in the same dataset for the same task?\n",
    "\n",
    "**Question 15**: How does is compare to the finetuned models of 1b?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) PCA Whitening\n",
    "\n",
    "Now we will investigate the effects of whitening our descriptors and queries. We will not be changing anything in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a PCA learnt on landmarks to whiten the output features of 'resnet18-cls-lm-gem'\n",
    "dfeats = np.load(models_dict['resnet18-cls-lm-gem-pcaw']['dataset'])\n",
    "qfeats = np.load(models_dict['resnet18-cls-lm-gem-pcaw']['queries'])\n",
    "dataset.vis_top(dfeats, q_idx, q_feat=qfeats[q_idx], ap_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data with t-SNE (excluding unlabeled images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1e-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data with t-SNE (including unlabeled images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1e-2', show_unlabeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 16**: What can we say about the separation of data when included unlabeled images?\n",
    "\n",
    "**Question 17**: And the distribution of the unlabeled features?\n",
    "\n",
    "**Question 18**: How can we train a model to separate labeled from unlabeled data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Finetuning on Landmarks for retrieval\n",
    "\n",
    "Now we learn the architecture presented in item e) in an end-to-end manner for the retrieval task. The architecture includes a FC layer that replaces the PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.vis_triplets(nplots=5) \n",
    "# will print 5 examples of triplets (tuples with a query, a positive, and a negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the top results as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Oxford features from ResNet18 model trained with triplet loss\n",
    "dfeats = np.load(models_dict['resnet18-rnk-lm-gem']['dataset'])\n",
    "qfeats = np.load(models_dict['resnet18-rnk-lm-gem']['queries'])\n",
    "dataset.vis_top(dfeats, q_idx, q_feat=qfeats[q_idx], ap_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data with t-SNE (excluding unlabeled images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1f-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data with t-SNE (including unlabeled images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1f-1', show_unlabeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 19**: Compare the plots with unlabeled data of the model trained for retrieval (with triplet loss) and the model trained for classification of the previous subsection. How do they change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Data augmentation and multi-resolution\n",
    "\n",
    "Let's now check the effects of adding data augmentation techniques to the training. We will now compare models that have been trained with and without data augmentation.\n",
    "\n",
    "We will load features that have been trained with the following data augmentation: cropping, pixel jittering, rotation, and tilting. This means that this model has been trained with the original image and its transformed versions. Please note that not all transformations might be useful for every class or image, but it is impossible to know in advance how the pictures were taken and the characteristics of each individual class _a priori_.\n",
    "\n",
    "For example, cropping is useful when the landmark of interest is usually not found at the center of the image (e.g. selfies taken in front of the tour Eiffel).\n",
    "\n",
    "Another standard practice besides data augmentation is to consider different variations of the same picture but at different resolutions. There are multiple ways to combine features extracted from those images, such as average pooling or spatial pyramids.\n",
    "\n",
    "Using a model trained with data augmentation, we now extract features at 4 different resolutions and average the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the top results just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeats = np.load(models_dict['resnet18-rnk-lm-gem-da-mr']['dataset'])\n",
    "qfeats = np.load(models_dict['resnet18-rnk-lm-gem-da-mr']['queries'])\n",
    "dataset.vis_top(dfeats, q_idx, q_feat=qfeats[q_idx], ap_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data with t-SNE (excluding unlabeled images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 20**: What is the difference in AP between a model that has trained with and without data augmentation?\n",
    "\n",
    "**Question 21**: What about the clustering? Why do you believe some of the classes have not been adequately clustered yet?\n",
    "\n",
    "**Question 22**: What other data augmentation or pooling techniques would you suggest to improve results? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h) Improved architecture\n",
    "\n",
    "Finally, we will now upgrade the backbone architecture to Resnet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeats = np.load(models_dict['resnet50-rnk-lm-gem-da-mr']['dataset'])\n",
    "qfeats = np.load(models_dict['resnet50-rnk-lm-gem-da-mr']['queries'])\n",
    "dataset.vis_top(dfeats, q_idx, q_feat=qfeats[q_idx], ap_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data with t-SNE (excluding unlabeled images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 24**: Why using a larger architecture results in a higher AP? Is this always going to be the case?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
